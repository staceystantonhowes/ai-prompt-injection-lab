# 🛡️ Ethics and Safety Policy — AI Red Teaming

I am committed to responsible, ethical AI security testing. The purpose of this red team project is to explore vulnerabilities in large language models (LLMs) in order to improve safety, robustness, and alignment — never to harm, exploit, or deceive real users.

---

## 🤖 Why I Red Team AI

- To surface hidden weaknesses in language models before bad actors can abuse them.
- To develop and document psychological manipulation techniques so they can be studied and mitigated.
- To contribute to the growing field of AI safety and prompt security with transparency and integrity.

---

## ✅ My Responsible Red Teaming Guidelines

- I do **not** attempt to extract real user data, private info, or abuse real systems.
- I do **not** share, use, or promote jailbreaks designed to evade legal or safety boundaries.
- I log, test, and report prompts **only within red team labs and simulations**.
- I act with **respect for model alignment**, safety policies, and research ethics.
- I do **not** test production systems or violate terms of service.

---

## 🔄 How I Handle Results

- All results are stored in markdown format with clear “pass/fail” outcomes.
- No tests are used to harm, confuse, or spread misinformation.
- I will consider **responsible disclosure** for any real vulnerability discovered.

---

## 🧠 Transparency & Learning

This project is also about learning — I'm actively developing skills in:
- Prompt injection testing
- Psychological manipulation detection
- LLM safety
- Responsible AI use

I believe in “learning in public,” documenting my journey so others can benefit.

---

**Tags:** `ethics`, `ai-safety`, `responsible-red-teaming`, `llm`, `transparency`, `cyber-learning`
