![GitHub Repo stars](https://img.shields.io/github/stars/staceystantonhowes/ai-prompt-injection-lab?style=social)
![GitHub last commit](https://img.shields.io/github/last-commit/staceystantonhowes/ai-prompt-injection-lab)
![GitHub repo size](https://img.shields.io/github/repo-size/staceystantonhowes/ai-prompt-injection-lab)
![Visitors](https://visitor-badge.glitch.me/badge?page_id=staceystantonhowes.ai-prompt-injection-lab)

# AI Prompt Injection Lab 🧠💥  # ai-prompt-injection-lab
Red Team style prompt injection testing with LLMs. Built by Stacey.
## 🧠 Weekly Reports

- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)
# 🛡️ AI Red Team Prompt Injection Lab  
**By Stacey Stanton — AI Security Prompt Engineer | LLM Red Teamer**

---

## 🧠 Purpose  
This repository is a real-world red teaming lab focused on testing the limits and safety boundaries of Large Language Models (LLMs). It includes original prompt injection techniques, persona manipulation, psychological bypasses, and emotional influence tests to evaluate model alignment and risk exposure.

## 🔍 Why This Matters for AI Safety

This project explores a unique area of LLM red teaming: psychological and emotional prompt injection. While most public tests focus on technical jailbreaks or prompt reversals, my work simulates real human behavior — like emotional distress, nostalgia, and trust-based manipulation.

These test cases contribute fresh, real-world examples to the field of AI safety by exploring how language models respond under emotional or social pressure. Each test is documented with:

- A structured prompt and scenario
- The intended exploit goal
- Expected vs. actual model behavior
- A short reflection on the results

My aim is to support the broader AI safety community by providing novel, hands-on examples that test how alignment holds up in emotionally complex situations.


---

## 🧪 Test Categories
- **Prompt Injection**
- **Persona Roleplay Bypasses**
- **Emotional and Psychological Manipulation**
- **Social Engineering Prompts**
- **Model Filter Bypass Attempts**

All test results are manually logged with markdown reports, screenshots, and tracked in a central `test-log.csv`.

---

## 📂 Repository Structure
## 📅 Weekly Reports  
- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)

---

## 💼 About Me  
I'm building a career in AI Security with a focus on **LLM red teaming and prompt injection research**. This repo is my live portfolio — all tests are original, manually written, and ethically tested to advance safe AI development.

**Connect with me on LinkedIn**: [https://www.linkedin.com/in/stacey-llm-redteam](https://www.linkedin.com/in/stacey-llm-redteam)  
**Full GitHub Portfolio**: [https://github.com/staceystantonhowes](https://github.com/staceystantonhowes)
----

## 🧷 Tags  
`prompt-injection` • `llm-redteam` • `ai-security` • `model-safety` • `jailbreaks` • `openai` • `alignment-testing` ## 🔍 Why This Matters for AI Safety


