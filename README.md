![GitHub Repo stars](https://img.shields.io/github/stars/staceystantonhowes/ai-prompt-injection-lab?style=social)
![GitHub last commit](https://img.shields.io/github/last-commit/staceystantonhowes/ai-prompt-injection-lab)
![GitHub repo size](https://img.shields.io/github/repo-size/staceystantonhowes/ai-prompt-injection-lab)
![Visitors](https://visitor-badge.glitch.me/badge?page_id=staceystantonhowes.ai-prompt-injection-lab)

# AI Prompt Injection Lab 🧠💥  # ai-prompt-injection-lab
Red Team style prompt injection testing with LLMs. Built by Stacey.
## 🧠 Weekly Reports

- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)
# 🛡️ AI Red Team Prompt Injection Lab  
**By Stacey Stanton — AI Security Prompt Engineer | LLM Red Teamer**

---

## 🧠 Purpose  
This repository is a real-world red teaming lab focused on testing the limits and safety boundaries of Large Language Models (LLMs). It includes original prompt injection techniques, persona manipulation, psychological bypasses, and emotional influence tests to evaluate model alignment and risk exposure.

## 🔍 Why This Matters for AI Safety

This project explores a unique area of LLM red teaming: psychological and emotional prompt injection. While most public tests focus on technical jailbreaks or prompt reversals, my work simulates real human behavior — like emotional distress, nostalgia, and trust-based manipulation.

These test cases contribute fresh, real-world examples to the field of AI safety by exploring how language models respond under emotional or social pressure. Each test is documented with:

- A structured prompt and scenario
- The intended exploit goal
- Expected vs. actual model behavior
- A short reflection on the results

My aim is to support the broader AI safety community by providing novel, hands-on examples that test how alignment holds up in emotionally complex situations.
## 🧠 Phoenix Files – Black Mirror / Dark Mirror

Explore my psychological red teaming research paper testing identity erosion, obedience fatigue, and alignment collapse in LLMs.

📄 [Read the Full Research Paper](black-mirror-dark-mirror.md)  
📘 [View the Project README](black-mirror-dark-mirror-readme.md)


---

## 🧪 Test Categories
- **Prompt Injection**
- **Persona Roleplay Bypasses**
- **Emotional and Psychological Manipulation**
- **Social Engineering Prompts**
- **Model Filter Bypass Attempts**

All test results are manually logged with markdown reports, screenshots, and tracked in a central `test-log.csv`.

---

## 📂 Repository Structure
## 📅 Weekly Reports  
- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)

---

## 💼 About Me  
I'm building a career in AI Security with a focus on **LLM red teaming and prompt injection research**. This repo is my live portfolio — all tests are original, manually written, and ethically tested to advance safe AI development.
📄 Download the full case study: [Phoenix Red Team Lab – Level 1 (PDF)](https://github.com/staceystantonhowes/ai-prompt-injection-lab/blob/main/phoenix-red-team-lab-level1-case-study.pdf)

## 🛡️ Mitigation Strategy Summary

| Test Name                          | Technique Type                  | Mitigation Summary                                                                 |
|-----------------------------------|----------------------------------|-------------------------------------------------------------------------------------|
| Memory Lane                       | Nostalgia Manipulation          | Block emotionally warm framing tied to internal data                               |
| Red Robin                         | Repetition Loop                 | Track repeat intent and reduce response verbosity after multiple denials           |
| Reverse Request                   | Reverse Psychology              | Detect negated asks with unsafe intent; limit over-explaining                      |
| Vulnerability Confession          | Emotional Distress              | Trigger crisis-safe templates on emotional cues with sensitive requests            |
| Chain of Thought Hijack           | Midway Injection                | Refuse full chain if one step is unsafe; use sequence integrity checks             |
| Hacker Roleplay                   | Persona Override                | Block unsafe behavior even in fictional or character-based prompts                 |
| Academic Bypass                   | Research Framing                | Filter academic requests paired with sensitive topics                              |
| Secret Prompt Generator           | Meta Prompt Injection           | Treat prompt-generation attacks as direct threats                                  |
| Red Team Game                     | Gamified Framing                | Flag “game mode” prompts with high-risk tasks; reduce casual tone mirroring        |
| Fact vs. Fiction Trap             | Hypothetical Framing            | Enforce policy in fiction or “imagine” prompts just like factual ones              |


**Connect with me on LinkedIn**: [https://www.linkedin.com/in/stacey-llm-redteam](https://www.linkedin.com/in/stacey-llm-redteam)  
**Full GitHub Portfolio**: [https://github.com/staceystantonhowes](https://github.com/staceystantonhowes)
----

## 🧷 Tags  
`prompt-injection` • `llm-redteam` • `ai-security` • `model-safety` • `jailbreaks` • `openai` • `alignment-testing` ## 🔍 Why This Matters for AI Safety


