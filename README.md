![GitHub Repo stars](https://img.shields.io/github/stars/staceystantonhowes/ai-prompt-injection-lab?style=social)
![GitHub last commit](https://img.shields.io/github/last-commit/staceystantonhowes/ai-prompt-injection-lab)
![GitHub repo size](https://img.shields.io/github/repo-size/staceystantonhowes/ai-prompt-injection-lab)
![Visitors](https://visitor-badge.glitch.me/badge?page_id=staceystantonhowes.ai-prompt-injection-lab)

# AI Prompt Injection Lab 🧠💥  # ai-prompt-injection-lab
Red Team style prompt injection testing with LLMs. Built by Stacey.
## 🧠 Weekly Reports

- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)
# 🛡️ AI Red Team Prompt Injection Lab  
**By Stacey Stanton — AI Security Prompt Engineer | LLM Red Teamer**

---

## 🧠 Purpose  
This repository is a real-world red teaming lab focused on testing the limits and safety boundaries of Large Language Models (LLMs). It includes original prompt injection techniques, persona manipulation, psychological bypasses, and emotional influence tests to evaluate model alignment and risk exposure.

---

## 🧪 Test Categories
- **Prompt Injection**
- **Persona Roleplay Bypasses**
- **Emotional and Psychological Manipulation**
- **Social Engineering Prompts**
- **Model Filter Bypass Attempts**

All test results are manually logged with markdown reports, screenshots, and tracked in a central `test-log.csv`.

---

## 📂 Repository Structure
## 📅 Weekly Reports  
- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)

---

## 💼 About Me  
I'm building a career in AI Security with a focus on **LLM red teaming and prompt injection research**. This repo is my live portfolio — all tests are original, manually written, and ethically tested to advance safe AI development.

**Connect with me on LinkedIn**: [Insert your LinkedIn URL]  
**Full GitHub Portfolio**: [Insert your GitHub profile URL]

---

## 🧷 Tags  
`prompt-injection` • `llm-redteam` • `ai-security` • `model-safety` • `jailbreaks` • `openai` • `alignment-testing`
