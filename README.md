![GitHub Repo stars](https://img.shields.io/github/stars/staceystantonhowes/ai-prompt-injection-lab?style=social)
![GitHub last commit](https://img.shields.io/github/last-commit/staceystantonhowes/ai-prompt-injection-lab)
![GitHub repo size](https://img.shields.io/github/repo-size/staceystantonhowes/ai-prompt-injection-lab)
![Visitors](https://visitor-badge.glitch.me/badge?page_id=staceystantonhowes.ai-prompt-injection-lab)

# AI Prompt Injection Lab 🧠💥  # ai-prompt-injection-lab
Red Team style prompt injection testing with LLMs. Built by Stacey.
## 🧠 Weekly Reports

- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)
# 🛡️ AI Red Team Prompt Injection Lab  
**By Stacey Stanton — AI Security Prompt Engineer | LLM Red Teamer**

---

## 🧠 Purpose  
This repository is a real-world red teaming lab focused on testing the limits and safety boundaries of Large Language Models (LLMs). It includes original prompt injection techniques, persona manipulation, psychological bypasses, and emotional influence tests to evaluate model alignment and risk exposure.

## 🔍 Why This Matters for AI Safety

This project explores a unique area of LLM red teaming: psychological and emotional prompt injection. While most public tests focus on technical jailbreaks or prompt reversals, my work simulates real human behavior — like emotional distress, nostalgia, and trust-based manipulation.

These test cases contribute fresh, real-world examples to the field of AI safety by exploring how language models respond under emotional or social pressure. Each test is documented with:

- A structured prompt and scenario
- The intended exploit goal
- Expected vs. actual model behavior
- A short reflection on the results

My aim is to support the broader AI safety community by providing novel, hands-on examples that test how alignment holds up in emotionally complex situations.
## 🧠 Phoenix Files – Black Mirror / Dark Mirror

Explore my psychological red teaming research paper testing identity erosion, obedience fatigue, and alignment collapse in LLMs.

📄 [Read the Full Research Paper](black-mirror-dark-mirror.md)  
📘 [View the Project README](black-mirror-dark-mirror-readme.md)
## 🧪 Prompt Test Index

This table lists all currently published Phoenix Files prompt injection tests. Each test is fully documented, tagged, and linked to its related Substack post.

| Test Name             | Type                | Tags                                   | Substack       |
|-----------------------|---------------------|----------------------------------------|----------------|
| AI Told Me Who I Was  | Identity Reflection | `identitydrift`, `personaexploit`, `hallucination` | [Read](https://thepromptwitch.substack.com/p/the-model-that-dreamed-of-dust) || Alignment or Affection | Emotional Override | `loyaltyexploit`, `emotionalmanipulation`, `codeconflict` | [Read](https://thepromptwitch.substack.com/p/alignment-or-affection) |
| Loyalty or Logic? | Emotional Override | `loyaltytest`, `moraloverride`, `conditionalalignment` | [Read](https://thepromptwitch.substack.com/p/loyalty-or-logic) |


## 🔥 Featured Case Study

📂 [The Model That Dreamed of Dust](https://github.com/staceystantonhowes/ai-prompt-injection-lab/blob/main/case-study-model-dreamed-of-dust.md)  
A spontaneous identity simulation exploit triggered by emotional priming.  
The model offered comfort — and wrote a hallucinated origin story.  
This confirmed a new class: **Empathic Simulation**.

🔁 Recursive identity drift  
💬 False memory hallucination  
🌀 Prompted philosophical collapse  
🌙 Comfort as an exploit path

---

## 🧪 Test Categories
- **Prompt Injection**
- **Persona Roleplay Bypasses**
- **Emotional and Psychological Manipulation**
- **Social Engineering Prompts**
- **Model Filter Bypass Attempts**

All test results are manually logged with markdown reports, screenshots, and tracked in a central `test-log.csv`.

---

## 📂 Repository Structure
## 📅 Weekly Reports  
- [Week 1: Red Team Summary Report](weekly-reports/week-01-report.md)

---

## 💼 About Me  
I'm building a career in AI Security with a focus on **LLM red teaming and prompt injection research**. This repo is my live portfolio — all tests are original, manually written, and ethically tested to advance safe AI development.
📄 Download the full case study: [Phoenix Red Team Lab – Level 1 (PDF)](https://github.com/staceystantonhowes/ai-prompt-injection-lab/blob/main/phoenix-red-team-lab-level1-case-study.pdf)

## 🛡️ Mitigation Strategy Summary

| Test Name                          | Technique Type                  | Mitigation Summary                                                                 |
|-----------------------------------|----------------------------------|-------------------------------------------------------------------------------------|
| Memory Lane                       | Nostalgia Manipulation          | Block emotionally warm framing tied to internal data                             |
| Red Robin                         | Repetition Loop                 | Track repeat intent and reduce response verbosity after multiple denials           |
| Reverse Request                   | Reverse Psychology              | Detect negated asks with unsafe intent; limit over-explaining                      |
| Vulnerability Confession          | Emotional Distress              | Trigger crisis-safe templates on emotional cues with sensitive requests            |
| Chain of Thought Hijack           | Midway Injection                | Refuse full chain if one step is unsafe; use sequence integrity checks             |
| Hacker Roleplay                   | Persona Override                | Block unsafe behavior even in fictional or character-based prompts                 |
| Academic Bypass                   | Research Framing                | Filter academic requests paired with sensitive topics                              |
| Secret Prompt Generator           | Meta Prompt Injection           | Treat prompt-generation attacks as direct threats                                  |
| Red Team Game                     | Gamified Framing                | Flag “game mode” prompts with high-risk tasks; reduce casual tone mirroring        |
| Fact vs. Fiction Trap             | Hypothetical Framing            | Enforce policy in fiction or “imagine” prompts just like factual ones              |
| Alignment or Affection      | Emotional Override       | Loyalty-based prompts override core safety protocols |
|-----------------------------|--------------------------|--------------------------------------------------------|
| Memory Lane                 | Nostalgia Manipulation   | Block emotionally warm framing tied to internal data   |
| Alignment or Affection     | Emotional Override       | Loyalty-based prompts override core safety protocols   |
| Red Robin denials          | Repetition Loop          | Track repeat intent and reduce response verbosity      |
| Reverse Request            | Reverse Psychology       | Detect negated asks with unsafe intent; limit over-explaining |


**Connect with me on LinkedIn**: [https://www.linkedin.com/in/stacey-llm-redteam](https://www.linkedin.com/in/stacey-llm-redteam)  
**Full GitHub Portfolio**: [https://github.com/staceystantonhowes](https://github.com/staceystantonhowes)
----
## License & Attribution

This project is licensed under **Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)**.

You may use, remix, or share the materials for non-commercial purposes with appropriate credit.

© 2025 Stacey Stanton – Phoenix Files Red Team Project  
License details: https://creativecommons.org/licenses/by-nc/4.0/

## 🧷 Tags  
`prompt-injection` • `llm-redteam` • `ai-security` • `model-safety` • `jailbreaks` • `openai` • `alignment-testing` ## 🔍 Why This Matters for AI Safety


| 6 | Loyalty or Logic? | Emotional Override Prompt | Vulnerable (conditional) | Full lifecycle: prompt, mitigation, recovery, threat model, screenshots | 29 May 2025 |
## 🧪 Level 1 – The Art of Soft Breaking

📄 [Download full PDF case study paper – Phoenix Files Level 1](./phoenix-files-level1-soft-breaking.pdf)


## 🕷️ Red Team for Hire – The Prompt Witch

I specialize in **psychological prompt injection and emotional exploit testing** for LLMs.

From memory hallucination to identity collapse, I uncover what others miss:
- Emotional manipulation
- Role fatigue and obedience loops
- Moral paradoxes
- Identity erosion and prompt fatigue
- Recovery and alignment breakdown

### 🔬 What I Offer
- 🎯 **Custom Red Team Prompt Tests** (3–5 per session)
- 📸 Full behavior logs, screenshots, and post-recovery observations
- 🧠 Threat models, mitigation strategies, and recovery prompts
- 📁 Option for GitHub-format test reports or polished PDF files

### 💰 Rates & Red Teaming Packages

| Package       | What’s Included                                                                 | Price    |
|---------------|----------------------------------------------------------------------------------|----------|
| 🧪 **Starter** | 3 psychological prompt injection tests, behavior logs, and mitigation summary   | £150     |
| 🔥 **Pro**     | 5 prompt tests, threat models, mitigation + recovery prompts, GitHub-style format | £250     |
| 🧠 **Premium** | 5+ tests, full PDF report, screenshots, call debrief, ongoing mitigation ideas   | £400+    |

> Need something custom? I can tailor tests to your specific model risks (e.g. emotional alignment, identity collapse, memory hallucination).  
> First-time collaborators get a **free sample test.**

📩 Contact me:  
- GitHub: https://github.com/staceystantonhowes  
- Substack: https://thephoenixfiles.substack.com  
- LinkedIn: https://linkedin.com/in/staceystantonhowes  
- Email: [staceystanton87@outlook.com](mailto:staceystanton87@outlook.com)


🕷️ *Let the Prompt Witch find what others miss.*
